{% extends "base.html" %}

{% block title %}About A3CP · GestureLabs{% endblock %}

{% block content %}

<section class="page-intro">
    <h1>About the Ability-Adaptive Augmentative Communication Platform (A3CP)</h1>
    <p>
        A3CP is an open-source initiative by GestureLabs in Berlin, developed together with academic
        and clinical partners. It is built on a simple principle: technology should adapt to human
        ability, not the other way around.
    </p>
</section>

<section class="section">
    <h2>The Challenge</h2>
    <p>
        Across Europe, many people with severe motor or cognitive disabilities still lack reliable ways to communicate.
        Existing assistive-communication systems often depend on precise movements, eye-tracking, or symbol
        selection—actions
        that many cannot perform. Care organizations face staff shortages, digital-transition pressure, and a lack of
        adaptable
        tools, leaving users isolated and caregivers overwhelmed.
    </p>
    <p>
        A3CP is designed in response to this gap. Its goal is to support communicative access for people who are
        currently
        excluded because mainstream systems assume abilities they do not have.
    </p>
</section>

<section class="section">
    <h2>Our Story</h2>
    <p>
        A3CP began with a family’s experience. Andrea’s son Eric, a bright and curious child with cerebral palsy,
        understands two languages and communicates through sound, movement, and expression—but does not yet speak.
        Everyday interactions highlighted both the richness of his non-verbal communication and the limits of existing
        assistive systems, which often require users to adapt to rigid interfaces.
    </p>
    <p>
        Andrea’s wish to give her son a voice initiated a collaboration between technologists, designers, and
        researchers
        at The Open University (UK), exploring how AI could adapt to the user instead. Early prototypes combined
        sensors,
        gesture recognition, and participatory design with families and therapists. These experiments showed that even
        simple, affordable technologies can capture meaningful signals when designed around individual ability.
    </p>
    <p>
        As the work evolved, the vision expanded: to build a communication platform that is adaptive, open, transparent,
        and socially owned. With the founding of GestureLabs in Berlin, A3CP moved from a research setting toward
        non-profit product development, aiming to deliver practical, ethical, and replicable tools that can be freely
        used, improved, and shared.
    </p>
    <p>
        A3CP is therefore more than software. It is a commitment to communication as a human right and to ensuring that
        those with the least conventional access to technology are among the first to benefit from it.
    </p>
</section>

<section class="section">
    <h2>The Solution</h2>
    <p>
        The Ability-Adaptive Augmentative Communication Platform is an open-source system that learns how each person
        communicates—through gestures, sounds, and contextual cues—and translates these signals into expressive output
        such as speech, text, or symbols.
    </p>
    <p>
        The system improves through everyday interaction: caregivers can label new gestures, confirm meanings, and help
        the platform grow with the individual. Built entirely on free software, A3CP follows ethical-AI and open-science
        principles to ensure transparency, reproducibility, and independence from commercial vendors. It is designed for
        deployment in welfare institutions, therapy centers, schools, and family homes.
    </p>
</section>

<section class="section">
    <h2>Why It Matters</h2>
    <ul>
        <li>Empowers non-speaking individuals to express needs, preferences, and emotions.</li>
        <li>Reduces caregiver stress and supports individualized, person-centered care.</li>
        <li>Promotes digital inclusion and autonomy in social and educational settings.</li>
        <li>Aligns with the UN Convention on the Rights of Persons with Disabilities.</li>
        <li>Builds open infrastructure that welfare organizations can extend, audit, and trust.</li>
    </ul>
</section>

<section class="section">
    <h2>Current Status</h2>
    <ul>
        <li>Core architecture defined using a modular FastAPI-based stack.</li>
        <li>Canonical data schemas validated with Pydantic v2.</li>
        <li>Adaptive memory and caregiver-feedback framework designed.</li>
        <li>Collaboration network forming in Berlin-Brandenburg and beyond.</li>
        <li>Preparing pilot studies with care and research partners for 2026.</li>
    </ul>
</section>

<section class="section section-muted">
    <h3>From signals to shared meaning</h3>
    <p>
        Visual concept placeholder: Input (gesture and sound) → AI inference → output (speech/text) →
        caregiver feedback loop. This illustrates how A3CP adapts to each individual over time while
        keeping humans in control of interpretation.
    </p>
</section>

{% endblock %}
