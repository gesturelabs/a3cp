{% extends "base.html" %}

{% block title %}Technology · A3CP by GestureLabs{% endblock %}

{% block content %}

<section class="page-intro">
    <h1>Technology</h1>
    <p>
        The Ability-Adaptive Augmentative Communication Platform (A3CP) translates complex research into a practical,
        open, and ethical communication system. This page outlines how the platform works — from signal capture to
        reasoning and feedback — and explains the modular architecture that makes A3CP adaptable and transparent.
    </p>
</section>

<section class="section">
    <h2>Overview</h2>
    <p>
        A3CP combines gesture and sound recognition with human feedback to create an ability-adaptive communication
        system.
        Unlike traditional AAC tools that depend on symbol or text selection, A3CP learns directly from each person’s
        expressive movements, sounds, and context. Its open-source design ensures transparency, ethical oversight, and
        adaptability across care, education, and research environments.
    </p>
</section>

{% set sbg_title="Modular Architecture" %} {% set sbg_intro %} A3CP is built as a modular framework: each component
operates independently and communicates through shared data standards. This makes the system easier to audit,
extend, and deploy across care, therapy, and educational environments. {% endset %} {% set sbg_items=[
{"title": "1. Capture Layer" , "text"
: "Camera and microphone inputs collected locally for low-latency, private processing." },
{"title": "2. Feature Extraction" , "text"
: "Landmarks, movement features, and audio features converted into compact vectors." },
{"title": "3. Classification" , "text" : "User-specific models generate intent predictions with confidence scores."
}, {"title": "4. CARE Engine" , "text"
: "Fuses predictions, checks uncertainty, and triggers caregiver clarification when needed." },
{"title": "5. Memory & Learning" , "text"
: "Stores caregiver-confirmed examples to continually adapt to the user’s patterns." },
{"title": "6. Interface Layer" , "text"
: "Outputs speech, text, or symbols; explains uncertainty; requests confirmation." } ] %} {%
include "components/_six_block_grid.html" %}

<section class="section">
    <h2>The CARE Engine</h2>
    <p>
        The CARE Engine — Clarification, Adaptation, Reasoning, and Explanation — is the reasoning core of A3CP.
        It fuses information from gesture, sound, and contextual modules, computes confidence levels, and decides
        when to ask for clarification. When confidence is low, it presents the most likely interpretations to
        caregivers for confirmation. Each clarification becomes new training data, enabling the system to learn
        and adapt to the individual user.
    </p>
</section>

<section class="section">
    <h2>Ethical and Edge-Capable Design</h2>
    <p>
        A3CP is engineered to run fully offline on affordable devices such as Raspberry Pi or Jetson Nano.
        This ensures privacy and accessibility for families, schools, and care homes, without dependence on
        commercial cloud services.
    </p>
    <p>
        Only derived feature data are stored—never raw video or audio—making learning processes transparent
        and explainable. All code and documentation are open-source so that anyone can review, replicate,
        or improve the platform.
    </p>
</section>

<section class="section">
    <h2>Development Path</h2>
    <ul>
        <li><strong>Phase 1:</strong> Streamlit demonstrator validated feasibility of gesture capture,
            visualization,
            and personalized training.</li>
        <li><strong>Phase 2:</strong> Modular FastAPI architecture established a scalable foundation for real-world
            deployment.</li>
        <li><strong>Phase 3 (2026):</strong> Integration of gesture and sound classifiers, caregiver-in-the-loop
            training, and pilot studies.</li>
    </ul>
</section>

<section class="section">
    <h2>Future Possibilities</h2>
    <p>
        The adaptive architecture of A3CP can support creative and educational uses — from composing music or art
        through gestures to connecting with digital avatars or providing learning analytics for therapists.
        Because every interaction is logged as structured, interpretable data, A3CP can also contribute to new
        research on communication development and multimodal expression.
    </p>
</section>

<section class="section section-muted">
    <h3>Visual concepts</h3>
    <ul>
        <li>Architecture Diagram: Input → Classification → CARE Engine → Output → Feedback loop.</li>
        <li>Clarification Loop Illustration: caregiver confirmation and model update cycle.</li>
        <li>Edge Deployment Graphic: Raspberry Pi or tablet with local processing arrows (no cloud icons).</li>
    </ul>
</section>

{% endblock %}
