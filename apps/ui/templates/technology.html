{# apps/ui/templates/technology.html #}
{% extends "base.html" %}

{% block title %}Technology · A3CP by GestureLabs{% endblock %}

{% block content %}

{# --- Intro section via centered text block --- #}
{% set ct_eyebrow = "Technology" %}
{% set ct_headline = "How A3CP turns multimodal signals into adaptive communication." %}
{% set ct_subheadline = "
The Ability-Adaptive Augmentative Communication Platform (A3CP) turns movements and sounds into communication with
help from caregivers. It is built from clear, separate parts so the system can be understood, checked, and improved over
time.
" %}
{% include "components/_centered_text_block.html" %}

{# --- End-to-End Pipeline Illustration --- #}
{% set ci_src = "/static/img/illustrations/Architecture_Diagram.png" %}
{% set ci_alt = "A3CP pipeline from multimodal inputs through classification, CARE Engine decision logic, output, and
feedback." %}
{% set ci_caption = "End-to-end A3CP pipeline: signals → features → classification → CARE Engine → output → feedback."
%}
{% set ci_muted = true %}
{% include "components/_centered_image.html" %}




{# --- Modular architecture via six-block grid --- #}
{% set sbg_title = "Modular Architecture" %}
{% set sbg_intro = "
A3CP is built as a modular framework: each component operates independently and communicates through shared data
standards. This makes the system easier to audit, extend, and deploy across care, therapy, and educational environments.
" %}

{% set sbg_items = [
{
"title": "1. Capture Layer",
"text": "Camera and microphone inputs collected locally for low-latency, privacy-preserving processing."
},
{
"title": "2. Feature Extraction",
"text": "Landmarks, movement features, and audio features converted into compact numeric vectors."
},
{
"title": "3. Classification",
"text": "User-specific models generate intent predictions with calibrated confidence scores."
},
{
"title": "4. CARE Engine",
"text": "Fuses predictions, checks uncertainty, and triggers caregiver clarification when needed."
},
{
"title": "5. Memory & Learning",
"text": "Stores caregiver-confirmed examples so the system gradually adapts to the user’s patterns."
},
{
"title": "6. Interface Layer",
"text": "Drives speech, text, or symbol output and can explain uncertainty or request confirmation."
}
] %}

{% include "components/_six_block_grid.html" %}



{# --- CARE Engine: two-column text with schematic --- #}

{% set headline = "The CARE Engine" %}

{% set body_html %}
<p>
    The CARE Engine (Clarification, Adaptation, Reasoning, and Explanation) is the decision core that
    turns raw classifier outputs into safe, interpretable communication.
</p>

<p>
    It fuses information from gesture, sound, and contextual modules, computes confidence, and decides
    when to ask for clarification. When confidence is low, it surfaces the most plausible interpretations
    for caregivers instead of committing to a single guess.
</p>

<p>
    Each clarification becomes a structured training example, allowing the system to adapt to the
    individual user over time while keeping humans in control of final decisions.
</p>
{% endset %}

{% set img_src = "/static/img/illustrations/Care_Engine.png" %}
{% set img_alt = "CARE Engine schematic showing fusion of gesture, sound, and context, confidence gating, clarification,
and memory updates." %}
{% set caption = "The CARE Engine uses confidence-aware reasoning and caregiver confirmation to guide safe adaptation."
%}
{% set reverse = false %}

{% include "components/_two_column_text_w_photo.html" %}


{# --- Ethical and Edge-Capable Design: two-column text with schematic --- #}

{% set headline = "Ethical and Edge-Capable Design" %}

{% set body_html %}
<p>
    A3CP is engineered to preserve privacy, remain inspectable, and support multiple deployment modes:
    cloud, edge-only, or hybrid edge with cloud-based updating.
</p>

<p>
    In edge deployments, inference runs locally on affordable devices such as Raspberry Pi or Jetson Nano.
    In hybrid mode, the system can run locally while optionally synchronizing model updates or configuration
    changes via a cloud service when connectivity is available.
</p>

<p>
    Only derived feature data are stored — never raw video or audio — so that learning and adaptation remain
    transparent and explainable. Code and documentation are open source, enabling independent review,
    replication, and improvement, and reducing vendor lock-in.
</p>
{% endset %}

{% set img_src = "/static/img/illustrations/Cloud_Edge.png" %}
{% set img_alt = "Diagram showing A3CP deployment options: cloud, edge-only, and hybrid edge with cloud-based updates."
%}
{% set caption = "Deployment modes: edge-only, cloud, or hybrid edge inference with optional cloud updating." %}
{% set reverse = false %}

{% include "components/_two_column_text_w_photo.html" %}






{# Development Path as three-card row #}
{% set tcr_title = "Development Path" %}
{% set tcr_intro = "A3CP has progressed through successive prototypes toward a stable, deployable system." %}

{% set tcr_cards = [
{
"title": "Phase 1",
"description": "Streamlit demonstrator validated feasibility of gesture capture, landmark visualization, and
personalized training.",
"href": "/docs"
},
{
"title": "Phase 2",
"description": "Modular FastAPI architecture established a scalable foundation for real-world deployment.",
"href": "/docs"
},
{
"title": "Phase 3 (2026)",
"description": "Integration of gesture and sound classifiers, caregiver-in-the-loop training, and early pilot studies.",
"href": "/docs"
}
] %}

{% include "components/_three_card_row.html" %}




{# --- Future Possibilities (intro) --- #}
{% set ct_eyebrow = "Vision" %}
{% set ct_headline = "Future Possibilities" %}
{% set ct_subheadline = "A3CP’s adaptive architecture can support creative, therapeutic, and research applications.
Because interactions can be represented as structured, interpretable data, the same pipeline that supports communication
can also support learning, creativity, and longitudinal insight." %}

{% include "components/_centered_text_block.html" %}


{# --- 1) Gesture-based music or art composition --- #}
{% set headline = "Gesture-based music or art composition" %}
{% set body_html %}
<p>
    Embodied signals can be mapped to musical and visual outputs, enabling creative expression through accessible,
    user-specific interactions rather than conventional instruments or fine motor control.
</p>
{% endset %}
{% set img_src = "/static/img/illustrations/Embodied_Creativity.png" %}
{% set img_alt = "A wheelchair user uses gestures to control music and art elements shown on a screen." %}
{% set caption = "Creative expression through gesture-to-parameter mapping." %}
{% set reverse = false %}
{% include "components/_two_column_text_w_photo.html" %}


{# --- 2) Integration with digital avatars and embodied interfaces --- #}
{% set headline = "Integration with digital avatars and embodied interfaces" %}
{% set body_html %}
<p>
    Interpreted intent can control avatars for online communication, allowing users to choose their
    representation while maintaining control of meaning and interaction across distance.
</p>
{% endset %}
{% set img_src = "/static/img/illustrations/Embodied_Avatars.png" %}
{% set img_alt = "A user’s embodied interaction drives an outward-facing avatar used for online communication." %}
{% set caption = "Chosen representation for online presence." %}
{% set reverse = true %}
{% include "components/_two_column_text_w_photo.html" %}


{# --- 3) Adaptive learning tools for therapy and education --- #}
{% set headline = "Adaptive learning tools for therapy and education" %}
{% set body_html %}
<p>
    The same uncertainty-aware feedback loop can support learning activities by adapting task difficulty, input
    mappings, and prompts to the individual, helping to learn new gestures and develop communication abilities.
</p>
{% endset %}
{% set img_src = "/static/img/illustrations/Learning_Tools.png" %}
{% set img_alt = "A learner and partner use an adaptive activity on a screen with structured confirmation." %}
{% set caption = "Adaptive tasks with partner-in-the-loop confirmation." %}
{% set reverse = false %}
{% include "components/_two_column_text_w_photo.html" %}


{# --- 4) Longitudinal analytics for clinical teams --- #}
{% set headline = "Longitudinal analytics for clinical teams" %}
{% set body_html %}
<p>
    Aggregated summaries over weeks and months can support clinical review by showing trends in interaction success,
    uncertainty rates, and confirmed intent.
</p>
{% endset %}
{% set img_src = "/static/img/illustrations/Longitudinal_Analytics.png" %}
{% set img_alt = "Clinical team reviewing longitudinal trends and summaries on a shared display." %}
{% set caption = "Trends and summaries for clinical review." %}
{% set reverse = true %}
{% include "components/_two_column_text_w_photo.html" %}


{# --- 5) Multimodal datasets for research on communication development --- #}
{% set headline = "Multimodal datasets for research on communication development" %}
{% set body_html %}
<p>
    Structured traces can support research datasets that link context, model outputs, clarification
    outcomes, and confirmed intent, enabling studies of communication development over time.
</p>
{% endset %}
{% set img_src = "/static/img/illustrations/Multimodal_Datasets.png" %}
{% set img_alt = "Panels representing multimodal dataset components derived from structured interaction traces." %}
{% set caption = "Multimodal datasets derived from structured interaction traces." %}
{% set reverse = false %}
{% include "components/_two_column_text_w_photo.html" %}





{% endblock %}
