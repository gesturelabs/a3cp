
============================================================
 A3CP UI DESIGN STRATEGY: PANEL STRUCTURE DISCUSSION
============================================================

Context:
The A3CP UI must support three main tasks:
  1. Record Action
  2. Train Model
  3. Try It (Live Inference)

Unclear whether to implement as:
  - (A) 3‚Äì4 separate panels/screens
  - (B) One unified interface with tabs or sections

------------------------------------------------------------
OPTION A: SEPARATE PANELS (ONE PER TASK)
------------------------------------------------------------

+ PROS:
  - Simple mental model: one screen = one task
  - Easier to guide novice users (e.g. caregivers)
  - Fewer elements per screen ‚Üí less visual clutter

+ CONS:
  - More clicks to move between tasks
  - Can't easily share form state (e.g. user_id)
  - Slower for experienced users

+ RECOMMENDED WHEN:
  - Caregivers need simplicity and focus
  - Tasks are performed in isolation (e.g., just record)

------------------------------------------------------------
OPTION B: UNIFIED PANEL WITH TABS OR SECTIONS
------------------------------------------------------------

+ PROS:
  - Seamless workflow (Record ‚Üí Train ‚Üí Try)
  - Shared state: user_id, session_id can persist across tabs
  - Easier to build live demo flows (e.g. kiosk mode)

+ CONS:
  - Slightly more complex to design and implement
  - Risks clutter if all controls shown at once

+ RECOMMENDED WHEN:
  - User flow involves continuous sessions
  - Power users or researchers test end-to-end
  - Used in structured sessions or demonstrations


------------------------------------------------------------
DESIGN INSTRUCTION FOR UI DESIGNER
------------------------------------------------------------

You may propose either layout structure:
  ‚Üí Separate panels OR
  ‚Üí Unified layout with tabbed/sectioned view

But:
- Keep core tasks (Record, Train, Try) visually distinct
- Prioritize:
    ‚Ä¢ clarity
    ‚Ä¢ mobile/tablet usability
    ‚Ä¢ low cognitive load
- Ensure all fields from BaseSchema are properly captured
- UI should gracefully handle start/stop of input streams

============================================================


============================================================
 A3CP UI SPECIFICATION - RECORD ACTION PANEL
============================================================

Version: 1.0
Date: 2025-08-07
Applies to: Demonstrator UI (MVP)
Schema: BaseSchema
Note: session_id is NOT set here ‚Äî it is filled by session_manager later

------------------------------------------------------------
 PURPOSE
------------------------------------------------------------
Allow a user or caregiver to record a single structured action
with associated metadata. All input modalities (camera, mic, etc.)
are activated together. This panel collects metadata fields
defined in BaseSchema.

------------------------------------------------------------
 UI FIELDS (visible to user)
------------------------------------------------------------

[ ] user_id          (text input or dropdown)        required
[ ] performer_id     (text input)                    optional

NOTE: The following fields are handled automatically:
  - schema_version   ‚Üí auto-filled, e.g., "1.0.1"
  - record_id        ‚Üí UUIDv4, generated by frontend or backend
  - timestamp        ‚Üí current UTC time w/ milliseconds
  - modality         ‚Üí always set to "multimodal" here
  - source           ‚Üí fixed value: "partner_ui"
  - session_id       ‚Üí left blank (to be filled later)

------------------------------------------------------------
 BUTTONS & CONTROLS
------------------------------------------------------------

[ Start Recording ]    ‚Üí activates camera/mic
[ Stop Recording ]     ‚Üí ends capture
[ Save & Submit ]      ‚Üí sends metadata + recorded input

Recording is linked to the generated record_id.

------------------------------------------------------------
 SYSTEM BEHAVIOR
------------------------------------------------------------

- Recording includes gesture, audio, and speech input
- Data is saved locally and/or uploaded after Stop
- Form locks while recording is active
- Required: user_id must be filled before Start is allowed

------------------------------------------------------------
 EXAMPLE GENERATED PAYLOAD (BaseSchema)
------------------------------------------------------------

{
  "schema_version": "1.0.1",
  "record_id": "07e4c9ff-9b8e-4d3e-bc7c-2b1b1731df56",
  "user_id": "elias01",
  "timestamp": "2025-06-15T12:34:56.789Z",
  "performer_id": "carer01",
  "modality": "multimodal",
  "source": "partner_ui"
  // session_id omitted here
}

------------------------------------------------------------
 DESIGN NOTES
------------------------------------------------------------

- Keep layout minimal: top-down form + wide buttons
- Do NOT show system fields (record_id, timestamp)
- Validate user_id before enabling "Start Recording"
- Show recording status (e.g. [‚óè Recording‚Ä¶]) clearly

============================================================



============================================================
 A3CP UI SPECIFICATION - TRAIN MODEL PANEL (v2.0)
============================================================

Version: 2.0
Date: 2025-08-07
Applies to: Demonstrator UI (MVP)
Purpose: Train a model based on selected action-labeled samples

------------------------------------------------------------
 USER PERSPECTIVE
------------------------------------------------------------

- The user is training ONE model for a given user and label set
- Each sample corresponds to one action (label)
- Each sample may include multiple types of input (gesture, sound, etc.)
- The system determines internally which models are trained

------------------------------------------------------------
 UI FIELDS
------------------------------------------------------------

[ ] user_id           (dropdown or prefilled)         required
[ ] label_filter      (multi-select, optional)        optional
      ‚Üí e.g. only show samples with labels: ["yes", "no"]

Sample Selection Area:
- Grouped by action label
- Each sample shows:
    ‚Ä¢ label (e.g. "yes")
    ‚Ä¢ available data types (e.g. üßç üé§)
    ‚Ä¢ timestamp or ID
    ‚Ä¢ [x] include checkbox (default: selected)

[ Train Model ]       ‚Üí starts training on selected samples

------------------------------------------------------------
 SAMPLE VIEW EXAMPLE

User: elias01

+-------------------------+     +-------------------------+
| Label: "yes"            |     | Label: "no"             |
| [x] Sample A üßç üé§       |     | [x] Sample D üßç          |
| [x] Sample B üßç          |     | [ ] Sample E üé§         |
+-------------------------+     +-------------------------+

Legend:
üßç = gesture data (landmarks)
üé§ = audio data (waveform)

Samples without usable data may be:
  ‚Üí Greyed out, or
  ‚Üí Marked with warning (e.g., ‚ö† no landmarks)

------------------------------------------------------------
 SYSTEM BEHAVIOR (HIDDEN FROM USER)
------------------------------------------------------------

- On clicking [ Train Model ]:
    ‚Üí For each modality (gesture, audio, speech):
        - Filter selected samples that contain relevant data
        - Train classifier for that modality (if enough data exists)
    ‚Üí Track which classifiers were trained
    ‚Üí Return unified summary

------------------------------------------------------------
 CONFIRMATION MESSAGE

After training completes:

[‚úî] "Training complete."
     ‚Üí Samples used: 42
     ‚Üí Labels: ["yes", "no", "more"]
     ‚Üí Classifiers trained:
         ‚Ä¢ gesture_classifier (28 samples)
         ‚Ä¢ sound_classifier (14 samples)

[ Try It Now ‚Üí ]

------------------------------------------------------------
 DESIGN NOTES
------------------------------------------------------------

- Keep interface unified ‚Äî no classifier dropdown
- Show what modalities each sample includes
- Allow class-wise sample selection (bulk select by label)
- Disable or warn on samples that cannot be used by any classifier
- Provide summary, but not per-model technical detail

------------------------------------------------------------
 ARCHITECTURE NOTES (INTERNAL)

- System handles routing of data to:
    ‚Ä¢ gesture_classifier
    ‚Ä¢ sound_classifier
    ‚Ä¢ speech_classifier (if available)

- Models are trained independently per modality
- UI only presents a unified model concept to user

============================================================


============================================================
 A3CP UI SPECIFICATION - TRY IT PANEL
============================================================

Version: 1.0
Date: 2025-08-07
Applies to: Demonstrator UI (MVP)
Purpose: Run live inference using the trained model

------------------------------------------------------------
 USER PERSPECTIVE
------------------------------------------------------------

- The user selects a trained model and interacts with it live
- Input is captured via camera and microphone automatically
- The system analyzes gestures, sounds, or both ‚Äî no need for user
  to select classifiers or modalities
- Real-time predictions are displayed with confidence scores

------------------------------------------------------------
 UI FIELDS
------------------------------------------------------------

[ ] user_id             (dropdown or prefilled)         required
[ ] model_version       (dropdown, optional)            optional
      ‚Üí Defaults to latest trained model

[ Start Inference ]     ‚Üí activates camera/mic, begins prediction
[ Stop Inference ]      ‚Üí ends live input stream

------------------------------------------------------------
 LIVE PANEL LAYOUT

+--------------------------------------------------------+
| [‚óè Camera Feed]         | Prediction: "yes" (94%)      |
|                         |                              |
|                         | Labels seen:                 |
|                         |   ‚Ä¢ yes (94%)                |
|                         |   ‚Ä¢ no (5%)                  |
|                         |   ‚Ä¢ help (1%)                |
+--------------------------------------------------------+

Below or alongside:
‚Ä¢ Audio waveform (optional)
‚Ä¢ Confidence bar
‚Ä¢ Raw output log (toggleable)

------------------------------------------------------------
 SYSTEM BEHAVIOR (ABSTRACTED FROM USER)
------------------------------------------------------------

- Camera and mic both activated on Start
- Input streams processed by all available classifiers:
    ‚Ä¢ gesture_classifier ‚Üí from video
    ‚Ä¢ sound_classifier   ‚Üí from audio
    ‚Ä¢ others as added

- Classifier outputs are merged into:
    ‚Üí A unified prediction
    ‚Üí Confidence breakdown
    ‚Üí Optional time-series log

- Inference runs continuously until stopped

------------------------------------------------------------
 FEEDBACK & DISPLAY RULES

‚Ä¢ Show live top prediction at all times (e.g. "more" - 92%)
‚Ä¢ Confidence bars or scores for alternate labels
‚Ä¢ Option to show:
    - visual overlays (e.g. landmark skeleton)
    - audio levels
    - "raw output" for developers

‚Ä¢ If no input is detected:
    - Show "Waiting for input..." message
    - Handle camera/mic errors gracefully

------------------------------------------------------------
 DESIGN NOTES

- Prioritize clean, responsive layout (mobile/tablet-friendly)
- Camera view and prediction view must be visible at all times
- Status indicator: [‚óè Running] / [‚èπ Stopped]
- Minimal controls during inference to reduce accidental clicks

------------------------------------------------------------
 EXAMPLE WORKFLOW

1. User loads panel
2. User clicks [ Start Inference ]
3. Camera + mic activate
4. User performs gesture or vocalization
5. Prediction is shown live
6. User clicks [ Stop Inference ] to end session

------------------------------------------------------------
 OPTIONAL FEATURES (Post-MVP)

‚Ä¢ Playback recording for analysis
‚Ä¢ Export inference log
‚Ä¢ Manual override: "Was this correct?" feedback
‚Ä¢ Session tagging for retention/review

============================================================
